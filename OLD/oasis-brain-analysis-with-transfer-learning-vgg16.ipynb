{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Alzheimerâ€™s Diagnostic with OASIS\nA dataset of MRI brain scans of Alzheimer's patients and healthy controls.","metadata":{}},{"cell_type":"markdown","source":"### Content\n<!-- Create content from the below markdowns heading  -->\n- [Abstract](#Abstract)\n- [Introduction](#Introduction)\n- [Materials & Methods](#Materials-&-Methods)\n- [Importing](#Importing)\n- [Load Data](#Load-Data)\n- [Processing Image Data](#Processing-Image-Data)\n- [Visualizing MRI Data](#Visualizing-MRI-Data)\n- [Data Preprocessing](#Data-Preprocessing)\n- [Data Analysis](#Data-Analysis)\n- [Metrics](#Metrics)\n- [Post Analysis](#Post-Analysis)\n- [Results](#results)\n- [Discussion](#discussion)\n- [Conclusion](#conclusions)\n- [References](#References)\n","metadata":{}},{"cell_type":"markdown","source":"### Abstract\n`Alzheimer` is a nervous system disease that affects memory and thinking abilities of humans. Doctors do not consider it to be curable but if detected at early stage its progression can be slowed.\n\n`Open Access Series of Imaging Studies (OASIS)` brain data which can be used for Alzheimer's disease detection. It includes `MRI(Magnetic Resonance Imaging)` scans of the brain, which can help in detecting change of structural in brain of person diagnosed with Alzheimer's disease.\n\nThe aim of the project is to detect Alzheimer's disease at early stage using OASIS brain data set. This project involves using implementing `machine learning techniques` and exploring different algorithms and methods to accurately `detect the disease` through the given data. This model will detect change of structure is specific part of `brain and abnormalities` that leads to Alzheimer's disease. \n\nThe results of the projects have a potential for the improvement in development of tools for diagnostics of the disease and further understand about the disease in details.","metadata":{}},{"cell_type":"markdown","source":"### Introduction\nAn estimated 40 million people, mostly older than 60 years, have `dementia` worldwide, and this figure is projected to double every 20 years, until at least 2050\\cite{SCHELTENS2016505}. `Dementia of Alzheimer's Type (DAT)` is the most common form of dementia, affecting 1 in 9 people over the age of 65 years and as many as 1 in 3 people over the age of 85 \\cite{popuri2020using}. Thus, its a major health concern among all the other modern health issues.\n\nOASIS contains MRI scans of brain imaging with `neuroimaging and related clinical data` which is publicly available for research and analysis. It contains data to understand brain and helps in developing treatment approaches for various brain related diseases including Alzheimer's disease.\n\nCurrently, diagnostics of Alzheimer's disease diagnosis relies on a combination of `clinical evaluations, cognitive assessments, and neuroimaging` techniques. However, the accuracy and reliability of existing diagnostic methods can be limited, especially in the early stages of the disease.\n\nDiagnostic the disease through machine learning models by utilizing the OASIS data set would be a better and much more effective way as the model will classify if the person's brain is normal or have `some patterns` that reflects presence of Alzheimer's disease.","metadata":{}},{"cell_type":"markdown","source":"### Materials & Methods\nThe data set consists of `MRI of 150 individuals` aged 60 to 96 years, all scanned in a similar environment. Everyone was scanned on two or more visits, separated by at least one year for 373 imaging sessions \\cite{10.1162/jocn.2009.21407}. This data set contains brain images and demographic data of the person being scanned.\n\n \n\nImplemented `Convolution Neural Network (CNN)` for image recognition and applied Transfer Learning on pre-trained `VGG16(Visual Geometry Group 16)` CNN model. VGG16 has 16 hidden layers and 14,714,688 parameters. The model has been trained on 1000 images of 1000 different categories with coloured images. The layers of the VGG16 model are freezed; so that the weights will not get updated in further training, the input layer is chopped off, and a new input layer is added for the current input data. After the pre-defined model(VGG16), the output is flattened and sent to a fully connected layer with 64 neurons with Leaky ReLU (alpha = 0.1) as the activation function. The final output layer consists of three neurons (One for each class) with softmax as the activation function.\n","metadata":{}},{"cell_type":"markdown","source":"### Importing\n#### Importing Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\nfrom IPython.display import HTML\nimport seaborn as sns\nfrom matplotlib.animation import FuncAnimation\nimport psutil\nimport gc\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\nimport nibabel as nib\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-10T06:02:50.828182Z","iopub.status.busy":"2023-06-10T06:02:50.827562Z","iopub.status.idle":"2023-06-10T06:02:52.000039Z","shell.execute_reply":"2023-06-10T06:02:51.998030Z","shell.execute_reply.started":"2023-06-10T06:02:50.828136Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to print bold text\ndef bold(text):\n    return f\"\\033[1m{text}\\033[0m\"\n    \n# It will fix the file path to the correct path\n# def clearFilePath(path):\n#     file_name = re.sub(\".*\" + \"/oasis2/\", '', path)\n#     file_name = re.sub('/', '_', file_name)\n#     file_name = re.sub('mpr-', '', file_name)\n#     file_name = re.sub('.nifti.img', '', file_name)\n#     return file_name\ndef clearFilePath(path):\n    file_name = path.split('\\\\')[-2:]\n    return file_name[0]+'_'+file_name[1].split('-')[-1][0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data\n#### Importing excel data","metadata":{}},{"cell_type":"code","source":"df_demographics_input = pd.read_excel('oasis_longitudinal_demographics.xlsx') \ndf_demographics_input\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:02:52.576326Z","iopub.status.busy":"2023-06-10T06:02:52.575802Z","iopub.status.idle":"2023-06-10T06:02:53.259682Z","shell.execute_reply":"2023-06-10T06:02:53.257463Z","shell.execute_reply.started":"2023-06-10T06:02:52.576285Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing image data.\nThis code is responsible for loading MRI \"nifti.img\" files from a specified directory, processing the data, and cleaning the data based on none values in the demographics.\nIt utilizes the `nibabel` library to load the MRI files and extract relevant information.","metadata":{}},{"cell_type":"code","source":"\n# rootdir = '../input/oasis2'\nrootdir = 'kaggle/input/oasis2'\n\n\nmri_patients_scans_name = []\nmri_ignored_file_names = []\nmri_images = []\nmri_images_data = []\n\n# Count the total number of files in the root directory\nfile_count = sum(len(files) for _, _, files in os.walk(rootdir))\n\nprint(f'Found {file_count} files in \"{rootdir}\" subdirectories\\n')\nprint('Loading MRI \"nifti.img\" files:')\n\n# Display a progress bar using tqdm\nwith tqdm(total=file_count) as pbar:\n    for subdir, dirs, files in os.walk(rootdir):\n        for file in files:\n            filepath = os.path.join(subdir, file)\n\n            if filepath.endswith(\"nifti.img\"):\n                try:\n                    # Load the MRI image using nibabel\n                    img = nib.load(filepath)\n                    mri_images.append(img)\n                    mri_patients_scans_name.append(clearFilePath(filepath))\n                    mri_images_data.append(img.get_fdata())\n                except nib.filebasedimages.ImageFileError as e:\n                    # If the file type is not recognized, ignore the file and print a message\n                    mri_ignored_file_names.append(filepath)\n                    print(\n                        f'File type not recognized - ignoring \"{filepath}\" file')\n\n            pbar.update(1)\n\nprint(\n    f'\\nFound and loaded {len(mri_patients_scans_name)} MRI \"nifti.img\" files from \"{rootdir}\" subdirectories')\nprint(f'Ignored:')\nprint(*mri_ignored_file_names, sep=' ')\n\n# Get unique patient visit names from MRI file names\nmri_patients_visits_names = np.unique(\n    [re.sub('_\\d\\Z', '', i) for i in mri_patients_scans_name])\n\n# Filter the demographics dataframe based on the MRI IDs\ndf_demographics = df_demographics_input[df_demographics_input['MRI ID'].isin(\n    mri_patients_visits_names)]\n\nprint(\"Before cleaning data from none values from demographics:\")\nprint(\n    f'Number of patients visits: {len(np.unique(mri_patients_visits_names))}')\nprint(f'Number of patients scans: {len(mri_patients_scans_name)}')\nprint(\n    f'Number of slices of MRI scans (unique values): {np.unique([arr.shape[2] for arr in mri_images_data])}')\n\n# Drop rows in demographics dataframe with None values\ndf_drop = df_demographics[df_demographics.isna().any(axis=1)]\nlist_drop = df_drop['MRI ID'].tolist()\nprint(\n    f'None value rows in demographics data to drop: {df_demographics.isnull().any(axis=1).sum()}')\ndf_demographics = df_demographics.dropna().reset_index(drop=True)\n\n# Remove corresponding MRI files and names based on dropped rows\nfname_drop = [fname for fname in mri_patients_scans_name if re.sub(\n    '_\\d\\Z', '', fname) in list_drop]\nfname_drop_id = [i for i in range(\n    len(mri_patients_scans_name)) if mri_patients_scans_name[i] in fname_drop]\n\nfor i in reversed(fname_drop_id):\n    del mri_images[i]\n    del mri_patients_scans_name[i]\n    del mri_images_data[i]\n\nprint(f\"After cleaning data from none values from demographics:\")\nprint(f\"Number of removed MRI files: {len(fname_drop)}\")\nprint(\n    f\"Number of patients visits: {len(np.unique(mri_patients_visits_names))}\")\nprint(f\"Number of patients scans: {len(mri_patients_scans_name)}\")\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:02:56.813761Z","iopub.status.busy":"2023-06-10T06:02:56.812885Z","iopub.status.idle":"2023-06-10T06:04:51.158363Z","shell.execute_reply":"2023-06-10T06:04:51.156625Z","shell.execute_reply.started":"2023-06-10T06:02:56.813718Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualizing MRI Data\nThis code segment performs visualizations of MRI slices using the matplotlib.pyplot library. It demonstrates the visualization of different slices from an MRI image.","metadata":{}},{"cell_type":"code","source":"i = np.transpose(mri_images_data[0], (1, 0, 2, 3))\n# Transpose the MRI image data to rearrange the dimensions for visualization\n\nplt.figure(figsize=(15, 5))\n# Create a figure with a size of 15x5 inches\n\nplt.subplot(131)\n# Define the first subplot in a 1x3 grid\n\nplt.imshow(np.transpose(\n    mri_images_data[0][:, :, 75], (1, 0, 2)), origin='lower')\n# Display the transposed MRI image slice at index 75\n# Adjust the dimensions for proper display using np.transpose()\n# Set the origin of the image to the lower-left corner\n\nplt.subplot(132)\n# Define the second subplot in the grid\n\nplt.imshow(i[120, :, :], origin='lower')\n# Display a different slice of the transposed MRI image (at index 120)\n\nplt.subplot(133)\n# Define the third subplot in the grid\n\nplt.imshow(i[:, 70, :], origin='lower')\n# Display another slice of the transposed MRI image (at index 70)\n\nplt.show()\n# Show the figure with the three subplots and MRI slice visualizations\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T05:41:11.610135Z","iopub.status.busy":"2023-06-10T05:41:11.609664Z","iopub.status.idle":"2023-06-10T05:41:12.032795Z","shell.execute_reply":"2023-06-10T05:41:12.031340Z","shell.execute_reply.started":"2023-06-10T05:41:11.610092Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, s = 40, 75\n# Define the image and slice indices to visualize\n\nimg_data = mri_images_data[image]\n# Retrieve the MRI image data for the specified image index\n\nimg_data = np.transpose(img_data, (1, 0, 2, 3))\n# Transpose the MRI image data to rearrange the dimensions for visualization\n\nmid_slice_x = img_data[:, :, s]\n# Extract the slice at index 's' from the transposed MRI image data\n\nmri_file_name = mri_patients_scans_name[image]\n# Retrieve the MRI file name corresponding to the specified image index\n\npatient_id = list(filter(None, re.split('_|MR', mri_file_name)))[1:]\n# Extract the patient ID from the MRI file name using regex\n\npatient_data = df_demographics[df_demographics['MRI ID'] == re.sub(\n    '_\\d\\Z', '', mri_file_name)]\n# Retrieve the demographics data for the patient ID obtained from the MRI file name\n\nplt.title(\n    f'MRI ID: {patient_id[0]} - visit: {patient_id[1]} - scan: {patient_id[2]}\\nGroup: {patient_data.iloc[0][2]}\\nSlice {s}')\n# Set the title of the plot with relevant information, such as MRI ID, visit, scan, group, and slice index\n\nplt.imshow(mid_slice_x, cmap='gray', origin='lower')\n# Display the extracted MRI slice using a grayscale colormap and with the origin set to the lower-left corner\n\nplt.colorbar(label='Signal intensity')\n# Add a colorbar to the plot with the label 'Signal intensity'\n\nplt.show()\n# Show the plot with the MRI slice visualization\n\npatient_data\n# Display the demographics data for the corresponding patient ID\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T05:41:39.907884Z","iopub.status.busy":"2023-06-10T05:41:39.907496Z","iopub.status.idle":"2023-06-10T05:41:40.147846Z","shell.execute_reply":"2023-06-10T05:41:40.146268Z","shell.execute_reply.started":"2023-06-10T05:41:39.907857Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slice_no = [120, 70, 70]\n# Define the slice numbers to visualize in each dimension\n\nfig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n# Create a figure with three subplots arranged in a single row\n\nax[0].imshow(img_data[:, :, slice_no[2]], origin='lower', cmap='gray')\nax[0].set_xlabel('Second dim voxel coords.', fontsize=12)\nax[0].set_ylabel('Third dim voxel coords', fontsize=12)\nax[0].set_title('First dimension', fontsize=15)\n# Display the slice along the first dimension in the first subplot\n# Set the x-axis and y-axis labels\n# Set the title for the subplot indicating the dimension\n\nax[1].imshow(img_data[slice_no[0], :, :], origin='lower', cmap='gray')\nax[1].set_xlabel('First dim voxel coords.', fontsize=12)\nax[1].set_ylabel('Third dim voxel coords', fontsize=12)\nax[1].set_title(f'Second dimension', fontsize=15)\n# Display the slice along the second dimension in the second subplot\n# Set the x-axis and y-axis labels\n# Set the title for the subplot indicating the dimension\n\nax[2].imshow(img_data[:, slice_no[1], :], origin='lower', cmap='gray')\nax[2].set_xlabel('First dim voxel coords.', fontsize=12)\nax[2].set_ylabel('Second dim voxel coords', fontsize=12)\nax[2].set_title(f'Third dimension', fontsize=15)\n# Display the slice along the third dimension in the third subplot\n# Set the x-axis and y-axis labels\n# Set the title for the subplot indicating the dimension\n\nfig.suptitle(\n    f'MRI ID: {patient_id[0]} - visit: {patient_id[1]} - scan: {patient_id[2]}\\nGroup: {patient_data.iloc[0][2]}\\nSlices: {slice_no}', fontsize=15)\n# Set the super-title of the figure with relevant information, such as MRI ID, visit, scan, group, and the selected slice numbers\n\nfig.tight_layout()\n# Adjust the subplots layout to prevent overlapping\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T05:41:43.234267Z","iopub.status.busy":"2023-06-10T05:41:43.233867Z","iopub.status.idle":"2023-06-10T05:41:43.836206Z","shell.execute_reply":"2023-06-10T05:41:43.834797Z","shell.execute_reply.started":"2023-06-10T05:41:43.234235Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df_demographics_input[df_demographics_input['Group']\n                != 'Converted'], y='eTIV', x='Age', hue='Group')\nplt.title(\"eTIV vs Age\")\n","metadata":{"execution":{"iopub.execute_input":"2023-06-07T20:02:02.042079Z","iopub.status.busy":"2023-06-07T20:02:02.041692Z","iopub.status.idle":"2023-06-07T20:02:02.354330Z","shell.execute_reply":"2023-06-07T20:02:02.352716Z","shell.execute_reply.started":"2023-06-07T20:02:02.042050Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.scatterplot(data=df_demographics_input[df_demographics_input['Group']\n                != 'Converted'], y='nWBV', x='Age', hue='Group')\nplt.title(\"nWBV vs Age\")\n","metadata":{"execution":{"iopub.execute_input":"2023-06-07T20:02:06.662641Z","iopub.status.busy":"2023-06-07T20:02:06.662278Z","iopub.status.idle":"2023-06-07T20:02:06.904452Z","shell.execute_reply":"2023-06-07T20:02:06.903851Z","shell.execute_reply.started":"2023-06-07T20:02:06.662617Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df_demographics_input['Group'])\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:05:35.992761Z","iopub.status.busy":"2023-06-10T06:05:35.990861Z","iopub.status.idle":"2023-06-10T06:05:36.245664Z","shell.execute_reply":"2023-06-10T06:05:36.244111Z","shell.execute_reply.started":"2023-06-10T06:05:35.992665Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mri_images_data[0].shape\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:05:42.229925Z","iopub.status.busy":"2023-06-10T06:05:42.229388Z","iopub.status.idle":"2023-06-10T06:05:42.238301Z","shell.execute_reply":"2023-06-10T06:05:42.236945Z","shell.execute_reply.started":"2023-06-10T06:05:42.229878Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve the patient details from the demographics data\ndef get_patient_detail(index):\n    mri_file_name = mri_patients_scans_name[index]\n    patient_data = df_demographics_input[df_demographics_input['MRI ID'] == re.sub(\n        '_\\d\\Z', '', mri_file_name)]\n    return patient_data\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:05:44.949870Z","iopub.status.busy":"2023-06-10T06:05:44.948452Z","iopub.status.idle":"2023-06-10T06:05:44.965596Z","shell.execute_reply":"2023-06-10T06:05:44.963222Z","shell.execute_reply.started":"2023-06-10T06:05:44.949805Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\nEncoding categorical label data in data preprocessing. This code segment demonstrates the use of the LabelEncoder classes to encode categorical data. \n\nAlso, we are selecting only three frame images from 3D image  dataset to reduce the computational time for the model training.Also VGG16 model is used for transfer learning and it only support only 3 channel images.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ncnn_y = []\n# Initialize an empty list to store the labels for CNN classification\n\nfor i in range(len(mri_images_data)):\n    # Iterate through each MRI image data\n\n    # Retrieve the patient group information using the 'get_patient_detail' function\n    patient_group = get_patient_detail(i)['Group'].values[0]\n\n    # Append the patient group to the 'cnn_y' list\n    cnn_y.append(patient_group)\n\n# Initialize a label encoder object for encoding the patient groups\n\n# Encode the patient groups using label encoding and convert them to number format\ncnn_y = to_categorical(label_encoder.fit_transform(cnn_y))\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:05:47.441224Z","iopub.status.busy":"2023-06-10T06:05:47.440682Z","iopub.status.idle":"2023-06-10T06:05:56.973012Z","shell.execute_reply":"2023-06-10T06:05:56.971336Z","shell.execute_reply.started":"2023-06-10T06:05:47.441176Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selects a subset of three consecutive slices (indexed 70, 71, and 72) from each MRI image in the mri_images_data array.\n# It then reshapes the subset to have a consistent shape of (256, 256, 3) representing an RGB image.\n# The resulting transformed data is stored in the mri_images_data_p numpy array for further processing or visualization.\nmri_images_data_p = np.array(\n    [i[:, :, [70, 100, 125]].reshape(256, 256, 3) for i in mri_images_data])\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:07:58.598325Z","iopub.status.busy":"2023-06-10T06:07:58.597826Z","iopub.status.idle":"2023-06-10T06:07:59.106708Z","shell.execute_reply":"2023-06-10T06:07:59.105087Z","shell.execute_reply.started":"2023-06-10T06:07:58.598286Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{bold('Length if image data: ')}{len(mri_images_data)}\")\nprint(f\"{bold('Length of y: ')}{len(cnn_y)}\")\nprint(f\"{bold('shape of a image: ')}{mri_images_data_p[0].shape}\")\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:08:01.083689Z","iopub.status.busy":"2023-06-10T06:08:01.083173Z","iopub.status.idle":"2023-06-10T06:08:01.094737Z","shell.execute_reply":"2023-06-10T06:08:01.093406Z","shell.execute_reply.started":"2023-06-10T06:08:01.083642Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"#### Importing pre-trained model\n\nVGG16 is a popular convolution's neural network architecture widely used for various computer vision tasks, including image classification. ImageNet is a large-scale dataset with millions of labeled images used for training deep learning models.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import applications\n# VGG16 pre-trained model without fully connected layers and with different input dimensions\ninput_shape = (256, 256, 3)\nmodel = applications.VGG16(\n    weights=\"imagenet\", include_top=False, input_shape=input_shape)\nmodel.summary()\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:08:04.650326Z","iopub.status.busy":"2023-06-10T06:08:04.649881Z","iopub.status.idle":"2023-06-10T06:08:06.120441Z","shell.execute_reply":"2023-06-10T06:08:06.119031Z","shell.execute_reply.started":"2023-06-10T06:08:04.650287Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze the models layers\nfor layer in model.layers:\n    layer.trainable = False\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:08:09.102450Z","iopub.status.busy":"2023-06-10T06:08:09.101027Z","iopub.status.idle":"2023-06-10T06:08:09.110421Z","shell.execute_reply":"2023-06-10T06:08:09.109087Z","shell.execute_reply.started":"2023-06-10T06:08:09.102385Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import LeakyReLU, Flatten, Dense\nfrom tensorflow.keras.models import Model\n\n# Flatten the output of the base model\nflatten = Flatten()(model.output)\n\n# Add a fully connected layer with 64 units\nfc1 = Dense(64)(flatten)\n\n# Apply LeakyReLU activation to introduce non-linearity\nleaky_relu = LeakyReLU(alpha=0.1)(fc1)\n\n# Add the final output layer with 3 units and softmax activation\noutput = Dense(3, activation='softmax')(leaky_relu)\n\n# Create a new model by specifying the input and output layers\nnew_model = Model(inputs=model.input, outputs=output)\n\n# Compile the new model with optimizer, loss function, and metrics\nnew_model.compile(optimizer='adam',\n                  loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print the summary of the new model\nnew_model.summary()\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:08:31.396326Z","iopub.status.busy":"2023-06-10T06:08:31.395829Z","iopub.status.idle":"2023-06-10T06:08:31.478234Z","shell.execute_reply":"2023-06-10T06:08:31.476681Z","shell.execute_reply.started":"2023-06-10T06:08:31.396287Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting data into training and test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#  split the data into training and testing sets\ntrain_X, test_X, train_y, test_y = train_test_split(\n    mri_images_data_p, cnn_y, test_size=.25)\n\nprint(f\"{bold('Train shape: ')}{train_X.shape}\")\nprint(f\"{bold('Test shape: ')}{test_X.shape}\")\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:08:43.235502Z","iopub.status.busy":"2023-06-10T06:08:43.234988Z","iopub.status.idle":"2023-06-10T06:08:43.469327Z","shell.execute_reply":"2023-06-10T06:08:43.468002Z","shell.execute_reply.started":"2023-06-10T06:08:43.235462Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training the model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# Define an early stopping callback with a patience of 6 epochs\nearly_stopping = EarlyStopping(patience=6)\n\n# Train the new model with early stopping\nhist = new_model.fit(train_X, train_y, validation_split=.2,\n                     epochs=20, callbacks=[early_stopping])\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:14:17.888465Z","iopub.status.busy":"2023-06-10T06:14:17.887441Z","iopub.status.idle":"2023-06-10T06:28:01.915057Z","shell.execute_reply":"2023-06-10T06:28:01.913447Z","shell.execute_reply.started":"2023-06-10T06:14:17.888408Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization of Accuracy and Validation loss of the model while training","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(hist.history).iloc[3:, [0, 2]].plot(\n    title=\"Loss vs Validation Loss\")\npd.DataFrame(hist.history).iloc[3:, [1, 3]].plot(\n    title=\"Accuracy vs Validaion Accuracy\")\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:32:56.059956Z","iopub.status.busy":"2023-06-10T06:32:56.059324Z","iopub.status.idle":"2023-06-10T06:32:56.578715Z","shell.execute_reply":"2023-06-10T06:32:56.577072Z","shell.execute_reply.started":"2023-06-10T06:32:56.059904Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prediction","metadata":{}},{"cell_type":"code","source":"# predictions on x_test dataset\ny_pred = np.argmax(new_model.predict(test_X), axis=1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics\n- Accuracy Score\n- F1 Score\n- Confusion Matrix\n- Classification Report","metadata":{}},{"cell_type":"code","source":"\nprint(f\"{bold('Accuracy Score: ')}{accuracy_score(np.argmax(test_y,axis=1),y_pred)}\\n\")\nprint(f\"{bold('Classification report')}{classification_report(np.argmax(test_y,axis=1),y_pred)}\\n\")\nprint(f\"{bold('F1 Score: ')}{f1_score(np.argmax(test_y,axis=1),y_pred, average=None)}\\n\")\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:36:43.790389Z","iopub.status.busy":"2023-06-10T06:36:43.789498Z","iopub.status.idle":"2023-06-10T06:37:02.025025Z","shell.execute_reply":"2023-06-10T06:37:02.023998Z","shell.execute_reply.started":"2023-06-10T06:36:43.790335Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CONFUSION MATRIX PLOT\nplt.figure(figsize=(8, 6))\n\n# Compute the confusion matrix\nc_m = confusion_matrix(np.argmax(test_y, axis=1), y_pred)\n\n# Plot the confusion matrix as a heatmap\nsns.heatmap(c_m, annot=True,cmap='Set2')\nplt.title(\"Confusion Matrix\")","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:38:23.939384Z","iopub.status.busy":"2023-06-10T06:38:23.938898Z","iopub.status.idle":"2023-06-10T06:38:24.251011Z","shell.execute_reply":"2023-06-10T06:38:24.249582Z","shell.execute_reply.started":"2023-06-10T06:38:23.939344Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the 4 images from the test dataset after prediction with its actual and predicted labels\n\nimages = [6, 10, 14, 26]\nplt.figure(figsize=(16, 12))\n\nfor index, image in enumerate(images):\n    img_data = test_X[image]\n\n    mri_file_name = mri_patients_scans_name[image]\n    true = label_encoder.classes_[np.argmax(test_y, axis=1)[image]]\n    actual = label_encoder.classes_[y_pred[image]]\n\n    plt.subplot(2, 2, index+1)\n    plt.title(f'\\nGroup: {true}\\nPredicted: {actual}')\n    plt.imshow(img_data[:, :, -1], cmap='gray', origin='lower')\n    plt.xlabel('First axis')\n    plt.ylabel('Second axis')\n    plt.colorbar(label='Signal intensity')\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2023-06-10T06:59:55.125843Z","iopub.status.busy":"2023-06-10T06:59:55.124888Z","iopub.status.idle":"2023-06-10T06:59:56.415441Z","shell.execute_reply":"2023-06-10T06:59:56.414053Z","shell.execute_reply.started":"2023-06-10T06:59:55.125792Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Post Analysis \nSaving the model weights ","metadata":{}},{"cell_type":"code","source":"new_model.save(\"OASIS_MODEL_1.h5\")","metadata":{"execution":{"iopub.execute_input":"2023-06-10T07:01:37.004210Z","iopub.status.busy":"2023-06-10T07:01:37.003646Z","iopub.status.idle":"2023-06-10T07:01:37.254768Z","shell.execute_reply":"2023-06-10T07:01:37.253730Z","shell.execute_reply.started":"2023-06-10T07:01:37.004166Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results \nThe data set has MRI images of `3-dimensional black-and-white images with shape (256, 256, 128)`. But, the pre-trained model used has been trained on images of `shape (224,224,3)`. Hence the data set was transformed into a shape in the format required for the model, and `specific frames (i.e., 75th, 100th and 125th)` were selected. The labels/target column (i.e., demented, non-demented and converted) were extracted from each `subject's Demographic (DM) data` for the respective images. These labels, along with the MRI images, were combined and used for training the model.  ","metadata":{}},{"cell_type":"markdown","source":"### Discussion \n\n`Convolution Neural Network(CNN)` is a popular algorithm for `image recognition` because it can detect patterns and objects in the image. We specifically decided to move ahead with VGG16, a pre-trained CNN model, as we had limited data sources, which would not have been enough to train a CNN from scratch due to its complex structure. \n\n \n\nFurther, it's very interesting to note that in the `OASIS2 brain dataset`, we have 72 subjects who identified as nondemented throughout the study, 64 subjects as demented in the initial visit and throughout the study, which also includes 51 subjects with mild to moderate Alzheimer's. The remaining 14 subjects were determined as non-demented in the initial visit and marked as demented in the later visits (converted). `These 14 subjects (converted) data is crucial for the problem statement, i.e., early detection, as their MRI samples tend to have patterns of the disease that looks like or develops in the early stages.`\n#### Evaluation \n\n    The model tends to classify the images as `demented, non-demented and converted with an accuracy of 94 percent`. Still, accuracy alone cannot give us the whole picture as accuracy metrics only considers the correctly predicted(True Positive + True Negative) output with respect to total outputs. Hence, it can give a false perception in case of unbalanced data. \n\n     \n\n    To overcome the limitation of accuracy score and as a requirement for the defined problem statement, we want to emphasize on reducing False Negative results, i.e. where a subject suffers from Alzheimer's, but the model detects otherwise. Therefore, the metrics we will focus on to evaluate the model are `Recall and F1 Score`. Recall, the fraction of the items of interest to the user retrieved by the system \\cite{alvarez2002exact}. The harmonic mean of precision and recall, F1 score, is widely used to measure the success of a classifier when one class is rare \\cite{lipton2014thresholding}. \n\n    `The Recall and F1 score for the given model is 95 percent and 92 percent, respectively. `  \n\n \n\n#### Limitation \n\n \n\nThe OASIS2 brain dataset consists of only 150 subjects and 373 MR sessions, all the subjects are aged between 60 and 96, and all the subjects are right-handed. We can see bias in the data as we do not have samples from young and left-handed subjects. \n\n \n\nMoreover, due to the data's complexity and large size(3d images), we could only train the model on limited subjects, as training the model on the whole data set would require greater computational power. \n\n \n\n  \n\n \n\n#### Future Scope \n\n \n\nThe future scope of the study would be to work with the latest `OASIS4 dataset`, which has more than `600 subjects and MR sessions`. But then, to process extensive data, we require robust systems that can process and transform the data. Further, we would like to collaborate with subject matter experts as they can assist us better in decision-making and understanding the problem. \n\n ","metadata":{}},{"cell_type":"markdown","source":"### Conclusions \n\nIn conclusion, the `study focuses on Alzheimer's disease detection at an early stage using image recognition with CNN.` This study can be used in the development of robust diagnostics tools. Additionally, it can further improve Alzheimer's disease detection and highlights the potential of machine learning and neuroimaging techniques in the advanced understanding of structural change in the brain.  ","metadata":{}},{"cell_type":"markdown","source":"### References\n- Data were provided by OASIS [Longitudinal: Principal Investigators: D. Marcus, R, Buckner, J. Csernansky, J. Morris; P50 AG05681, P01 AG03991, P01 AG026276, R01 AG021910, P20 MH071616, U24 RR021382](https://doi.org/10.1162/jocn.2009.21407)\n- Philip Scheltens, Ka j Blennow, Monique M B Breteler, Bart de Strooper, Giovanni B Frisoni, Stephen Salloway, and Wiesje Maria Van der Flier. Alzheimerâ€™s disease. The Lancet, 388(10043):505â€“517, 2016.\n- Karteek Popuri, Da Ma, Lei Wang, and Mirza Faisal Beg. Using machine learn- ing to quantify structural mri neurodegen- eration patterns of alzheimerâ€™s disease into dementia score: Independent validation on 8,834 images from adni, aibl, oasis, and miriad databases. Human Brain Mapping, 41(14):4127â€“4147, 2020.\n- Daniel S. Marcus, Anthony F. Fotenos, John G. Csernansky, John C. Morris, and Randy L. Buckner. Open Access Se- ries of Imaging Studies: Longitudinal MRI Data in Nondemented and Demented Older Adults. Journal of Cognitive Neuroscience, 22(12):2677â€“2684, 12 2010.\n- Sergio A Alvarez. An exact analytical re- lation among recall, precision, and classi- fication accuracy in information retrieval. Boston College, Boston, Technical Report BCCS-02-01, pages 1â€“22, 2002.\n- Zachary Chase Lipton, Charles Elkan, and Balakrishnan Narayanaswamy. Threshold- ing classifiers to maximize f1 score. arXiv preprint arXiv:1402.1892, 2014.","metadata":{}}]}